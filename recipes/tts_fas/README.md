# Towards Automatic Full Assessment of Text-to-Speech Systems

This is the recipe used for the ASR evaluation in the "Towards Automatic Full Assessment of Text-to-Speech Systems" paper.

Basically, the scripts here allow to evaluate the WER/TER of speech utterances generated by a speech synthesizer. Specifically, the idea is that the same synthesized samples are generated at each checkpoint while training the synthesizer, so these are evaluated by the ASR. This way, WER/TER scores are obtained at each checkpoint, and these error scores should decrease while training until reaching a plateau, giving an idea of how the synthesizer intelligibility improves as training advances.

Current scripts are specific to our experiments settings, but we plan to flexibilize the code to adapt to a variety of scenarios.

## Preparing the setup

First of all, you will need to generate synthesized audio samples for every checkpoint you want to evaluate, and store them in the same directory, with a folder for every checkpoint. For instance, we have generated around 129 utterances from the LJSpeech corpus at every checkpoint, all stored within "generated_speech_vocgan_resamp_16khz_16bits" directory.

```
tree generated_speech_vocgan_resamp_16khz_16bits -L 1
generated_speech_vocgan_resamp_16khz_16bits
├── 100000
├── 100800
├── 101600
├── 102400
```

